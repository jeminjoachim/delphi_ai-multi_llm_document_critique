{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7772605aanzJ",
        "outputId": "d2ee3794-c5a0-47ff-a9f5-3facbfdc9172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install cerebras-cloud-sdk --quiet\n",
        "!pip install PyPDF2 --quiet\n",
        "!pip install python-docx --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from cerebras.cloud.sdk import Cerebras"
      ],
      "metadata": {
        "id": "p_7oaA4RczcP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CEREBRAS_API_KEY\"] = \"Your API Key here\"\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = \"Your API Key here\"\n",
        "\n",
        "client = Cerebras(api_key=os.environ[\"CEREBRAS_API_KEY\"])"
      ],
      "metadata": {
        "id": "6_zr-HUXcEhU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Query Functions\n",
        "\n",
        "def query_gpt_oss(prompt, max_tokens=800):\n",
        "    \"\"\"GPT-OSS via Cerebras - used for synthesis\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-oss-120b\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def query_llama(prompt, max_tokens=500):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def query_deepseek(prompt, max_tokens=500):\n",
        "    url = \"https://api.deepseek.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {os.environ['DEEPSEEK_API_KEY']}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"deepseek-chat\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "    resp = requests.post(url, headers=headers, json=data)\n",
        "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "def query_qwen(prompt, max_tokens=500):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"qwen-3-235b-a22b-instruct-2507\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "fy5LF9facu9s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loading\n",
        "\n",
        "def load_document(path):\n",
        "    ext = path.split(\".\")[-1].lower()\n",
        "\n",
        "    if ext == \"txt\":\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "\n",
        "    elif ext == \"pdf\":\n",
        "        import PyPDF2\n",
        "        reader = PyPDF2.PdfReader(open(path, \"rb\"))\n",
        "        return \"\\n\".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "    elif ext == \"docx\":\n",
        "        import docx\n",
        "        doc = docx.Document(path)\n",
        "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {ext}\")"
      ],
      "metadata": {
        "id": "otLuFHL1dGTC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delphi-Style Iterative Critique System\n",
        "\n",
        "def generate_initial_critiques(document_text, task_instruction):\n",
        "    \"\"\"ROUND 1: Each LLM provides independent critique\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"You are an expert critic evaluating a document.\n",
        "\n",
        "--- DOCUMENT START ---\n",
        "{document_text}\n",
        "--- DOCUMENT END ---\n",
        "\n",
        "Task: {task_instruction}\n",
        "\n",
        "Provide a thorough critique covering:\n",
        "1. Strengths of the document\n",
        "2. Weaknesses and gaps\n",
        "3. Specific areas for improvement\n",
        "4. Overall assessment\n",
        "\n",
        "Be specific and actionable in your feedback.\"\"\"\n",
        "\n",
        "    print(\"ROUND 1: Generating initial independent critiques...\")\n",
        "\n",
        "    critiques = {}\n",
        "\n",
        "    # Get critique from each model\n",
        "    try:\n",
        "        print(\"  - Querying Llama...\")\n",
        "        critiques[\"llama\"] = query_llama(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"    Error with Llama: {e}\")\n",
        "        critiques[\"llama\"] = None\n",
        "\n",
        "    try:\n",
        "        print(\"  - Querying DeepSeek...\")\n",
        "        critiques[\"deepseek\"] = query_deepseek(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"    Error with DeepSeek: {e}\")\n",
        "        critiques[\"deepseek\"] = None\n",
        "\n",
        "    try:\n",
        "        print(\"  - Querying Qwen...\")\n",
        "        critiques[\"qwen\"] = query_qwen(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"    Error with Qwen: {e}\")\n",
        "        critiques[\"qwen\"] = None\n",
        "\n",
        "    return critiques"
      ],
      "metadata": {
        "id": "fQHjmRLudL5V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_refined_critiques(document_text, task_instruction, round1_critiques):\n",
        "    \"\"\"ROUND 2: Each LLM sees others' critiques and refines their own (Delphi iteration)\"\"\"\n",
        "\n",
        "    # Prepare summary of other critiques for each model\n",
        "    print(\"\\nROUND 2: Generating refined critiques after seeing others' feedback...\")\n",
        "\n",
        "    refined_critiques = {}\n",
        "\n",
        "    # Filter valid critiques from round 1\n",
        "    valid_critiques = {k: v for k, v in round1_critiques.items() if v is not None}\n",
        "\n",
        "    for model in [\"llama\", \"deepseek\", \"qwen\"]:\n",
        "        if model not in valid_critiques:\n",
        "            refined_critiques[model] = None\n",
        "            continue\n",
        "\n",
        "        # Gather OTHER models' critiques for this model to review\n",
        "        other_critiques = \"\"\n",
        "        for other_model, critique in valid_critiques.items():\n",
        "            if other_model != model:\n",
        "                other_critiques += f\"\\n\\n=== CRITIQUE FROM {other_model.upper()} ===\\n{critique}\"\n",
        "\n",
        "        prompt = f\"\"\"You are an expert critic. You previously evaluated a document.\n",
        "\n",
        "--- DOCUMENT START ---\n",
        "{document_text}\n",
        "--- DOCUMENT END ---\n",
        "\n",
        "Task: {task_instruction}\n",
        "\n",
        "YOUR INITIAL CRITIQUE:\n",
        "{round1_critiques[model]}\n",
        "\n",
        "OTHER EXPERTS' CRITIQUES:\n",
        "{other_critiques}\n",
        "\n",
        "Now that you've seen what other experts identified, please:\n",
        "1. Refine your initial critique\n",
        "2. Add any important points you may have missed\n",
        "3. Address any contradictions or different perspectives\n",
        "4. Strengthen your recommendations based on the collective insights\n",
        "5. Keep what was valuable in your original critique\n",
        "\n",
        "Provide your REFINED critique below:\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(f\"  - Refining {model.capitalize()}'s critique...\")\n",
        "\n",
        "            if model == \"llama\":\n",
        "                refined_critiques[model] = query_llama(prompt, max_tokens=600)\n",
        "            elif model == \"deepseek\":\n",
        "                refined_critiques[model] = query_deepseek(prompt, max_tokens=600)\n",
        "            elif model == \"qwen\":\n",
        "                refined_critiques[model] = query_qwen(prompt, max_tokens=600)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Error refining {model}: {e}\")\n",
        "            # Fall back to original critique\n",
        "            refined_critiques[model] = round1_critiques[model]\n",
        "\n",
        "    return refined_critiques"
      ],
      "metadata": {
        "id": "HD1re1RGj0TJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def synthesize_final_critique_gpt_oss(critiques_round1, critiques_round2):\n",
        "    \"\"\"Synthesize using GPT-OSS - shows evolution from Round 1 to Round 2\"\"\"\n",
        "\n",
        "    valid_r1 = {k: v for k, v in critiques_round1.items() if v is not None}\n",
        "    valid_r2 = {k: v for k, v in critiques_round2.items() if v is not None}\n",
        "\n",
        "    if not valid_r2:\n",
        "        return \"Error: No valid refined critiques were generated.\"\n",
        "\n",
        "    # Format critiques showing progression\n",
        "    critique_text = \"=== ROUND 1: INITIAL INDEPENDENT CRITIQUES ===\\n\"\n",
        "    for model, critique in valid_r1.items():\n",
        "        critique_text += f\"\\n--- {model.upper()} ---\\n{critique}\\n\"\n",
        "\n",
        "    critique_text += \"\\n\\n=== ROUND 2: REFINED CRITIQUES (AFTER SEEING OTHERS) ===\\n\"\n",
        "    for model, critique in valid_r2.items():\n",
        "        critique_text += f\"\\n--- {model.upper()} ---\\n{critique}\\n\"\n",
        "\n",
        "    synthesis_prompt = f\"\"\"You are synthesizing a Delphi-style expert panel critique process.\n",
        "\n",
        "The document was evaluated in TWO ROUNDS:\n",
        "- Round 1: Independent critiques\n",
        "- Round 2: Refined critiques after experts saw each other's feedback\n",
        "\n",
        "{critique_text}\n",
        "\n",
        "Your task:\n",
        "Create a comprehensive FINAL CRITIQUE that:\n",
        "1. Integrates insights from all experts across both rounds\n",
        "2. Highlights consensus points (what all experts agreed on)\n",
        "3. Notes areas where perspectives evolved between rounds\n",
        "4. Resolves any remaining contradictions thoughtfully\n",
        "5. Provides clear, prioritized, actionable recommendations\n",
        "6. Shows how the iterative process strengthened the analysis\n",
        "\n",
        "Structure your synthesis as:\n",
        "- Executive Summary\n",
        "- Consensus Findings (what all experts agreed on)\n",
        "- Key Evolution from Round 1 to Round 2\n",
        "- Critical Weaknesses Identified\n",
        "- Priority Recommendations\n",
        "- Conclusion\n",
        "\n",
        "Be thorough, insightful, and actionable.\"\"\"\n",
        "\n",
        "    print(\"\\nSYNTHESIS: Using GPT-OSS to create final unified critique...\")\n",
        "\n",
        "    try:\n",
        "        final_critique = query_gpt_oss(synthesis_prompt, max_tokens=1200)\n",
        "        return final_critique\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GPT-OSS synthesis: {e}\")\n",
        "        return \"Error: Failed to synthesize final critique.\""
      ],
      "metadata": {
        "id": "0cB_SnfAkAMB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delphi_critique_system(document_path, task_instruction):\n",
        "    \"\"\"Main Delphi-style iterative critique system\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DELPHI-STYLE MULTI-LLM CRITIQUE SYSTEM\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Step 1: Load document\n",
        "    print(f\"Loading document: {document_path}\")\n",
        "    try:\n",
        "        document_text = load_document(document_path)\n",
        "        print(f\"  ✓ Loaded ({len(document_text)} characters)\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error loading document: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Step 2: Round 1 - Independent critiques\n",
        "    critiques_round1 = generate_initial_critiques(document_text, task_instruction)\n",
        "\n",
        "    # Step 3: Round 2 - Refined critiques after seeing others\n",
        "    critiques_round2 = generate_refined_critiques(\n",
        "        document_text,\n",
        "        task_instruction,\n",
        "        critiques_round1\n",
        "    )\n",
        "\n",
        "    # Step 4: Final synthesis using GPT-OSS\n",
        "    final_critique = synthesize_final_critique_gpt_oss(\n",
        "        critiques_round1,\n",
        "        critiques_round2\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DELPHI PROCESS COMPLETE\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"round1_critiques\": critiques_round1,\n",
        "        \"round2_critiques\": critiques_round2,\n",
        "        \"final_critique\": final_critique\n",
        "    }"
      ],
      "metadata": {
        "id": "btheZZkDmb4r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save functions\n",
        "\n",
        "def save_results(result, output_file=\"critique_results\", format=\"txt\"):\n",
        "    \"\"\"Save results in txt, docx, or pdf format\"\"\"\n",
        "\n",
        "    if format == \"txt\":\n",
        "        _save_as_txt(result, f\"{output_file}.txt\")\n",
        "    elif format == \"docx\":\n",
        "        _save_as_docx(result, f\"{output_file}.docx\")\n",
        "    elif format == \"pdf\":\n",
        "        _save_as_pdf(result, f\"{output_file}.pdf\")\n",
        "    else:\n",
        "        print(f\"Unknown format: {format}. Defaulting to txt.\")\n",
        "        _save_as_txt(result, f\"{output_file}.txt\")\n",
        "\n",
        "def _save_as_txt(result, filename):\n",
        "    \"\"\"Save as plain text file\"\"\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(\"DELPHI-STYLE MULTI-LLM CRITIQUE RESULTS\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "\n",
        "        # Round 1 critiques\n",
        "        f.write(\"ROUND 1: INITIAL INDEPENDENT CRITIQUES\\n\")\n",
        "        f.write(\"-\"*60 + \"\\n\\n\")\n",
        "\n",
        "        for model, critique in result[\"round1_critiques\"].items():\n",
        "            f.write(f\"\\n{'='*40}\\n\")\n",
        "            f.write(f\"{model.upper()} - ROUND 1\\n\")\n",
        "            f.write(f\"{'='*40}\\n\\n\")\n",
        "            f.write(critique if critique else \"No critique generated\\n\")\n",
        "\n",
        "        # Round 2 critiques\n",
        "        f.write(\"\\n\\n\" + \"=\"*60 + \"\\n\")\n",
        "        f.write(\"ROUND 2: REFINED CRITIQUES (AFTER FEEDBACK)\\n\")\n",
        "        f.write(\"-\"*60 + \"\\n\\n\")\n",
        "\n",
        "        for model, critique in result[\"round2_critiques\"].items():\n",
        "            f.write(f\"\\n{'='*40}\\n\")\n",
        "            f.write(f\"{model.upper()} - ROUND 2 (REFINED)\\n\")\n",
        "            f.write(f\"{'='*40}\\n\\n\")\n",
        "            f.write(critique if critique else \"No refined critique\\n\")\n",
        "\n",
        "        # Final synthesized critique\n",
        "        f.write(\"\\n\\n\" + \"=\"*60 + \"\\n\")\n",
        "        f.write(\"FINAL SYNTHESIS (GPT-OSS)\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "        f.write(result[\"final_critique\"])\n",
        "\n",
        "    print(f\"\\n✓ Results saved to {filename}\")\n",
        "\n",
        "def _save_as_docx(result, filename):\n",
        "    \"\"\"Save as Word document with formatting\"\"\"\n",
        "    from docx import Document\n",
        "    from docx.shared import Pt, RGBColor\n",
        "    from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "\n",
        "    doc = Document()\n",
        "\n",
        "    # Title\n",
        "    title = doc.add_heading(\"Delphi-Style Multi-LLM Critique Results\", 0)\n",
        "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "    doc.add_paragraph()\n",
        "\n",
        "    # Round 1\n",
        "    doc.add_heading(\"Round 1: Initial Independent Critiques\", 1)\n",
        "    for model, critique in result[\"round1_critiques\"].items():\n",
        "        doc.add_heading(f\"{model.upper()} - Round 1\", 2)\n",
        "        doc.add_paragraph(critique if critique else \"No critique generated\")\n",
        "        doc.add_paragraph()\n",
        "\n",
        "    doc.add_page_break()\n",
        "\n",
        "    # Round 2\n",
        "    doc.add_heading(\"Round 2: Refined Critiques (After Feedback)\", 1)\n",
        "    for model, critique in result[\"round2_critiques\"].items():\n",
        "        doc.add_heading(f\"{model.upper()} - Round 2 (Refined)\", 2)\n",
        "        doc.add_paragraph(critique if critique else \"No refined critique\")\n",
        "        doc.add_paragraph()\n",
        "\n",
        "    doc.add_page_break()\n",
        "\n",
        "    # Final\n",
        "    doc.add_heading(\"Final Synthesis (GPT-OSS)\", 1)\n",
        "    doc.add_paragraph(result[\"final_critique\"])\n",
        "\n",
        "    doc.save(filename)\n",
        "    print(f\"\\n✓ Results saved to {filename}\")\n"
      ],
      "metadata": {
        "id": "_Xs1oeVqkqJJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Implementation\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    result = delphi_critique_system(\n",
        "        document_path=\"/content/The Intelligence Age (Sep 2024).pdf\",\n",
        "        task_instruction=\"Critically evaluate this research document and identify weaknesses, gaps, and areas for improvement.\"\n",
        "    )\n",
        "\n",
        "    if result:\n",
        "        # Print final critique\n",
        "        print(\"\\nFINAL CRITIQUE:\")\n",
        "        print(\"-\"*60)\n",
        "        print(result[\"final_critique\"])\n",
        "\n",
        "        save_results(result, \"delphi_critique\", format=\"docx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jcHP_cfeXhC",
        "outputId": "635f21b3-0deb-4420-b2ad-26a6142c0f6d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DELPHI-STYLE MULTI-LLM CRITIQUE SYSTEM\n",
            "============================================================\n",
            "\n",
            "Loading document: /content/The Intelligence Age (Sep 2024).pdf\n",
            "  ✓ Loaded (6507 characters)\n",
            "\n",
            "ROUND 1: Generating initial independent critiques...\n",
            "  - Querying Llama...\n",
            "  - Querying DeepSeek...\n",
            "  - Querying Qwen...\n",
            "\n",
            "ROUND 2: Generating refined critiques after seeing others' feedback...\n",
            "  - Refining Llama's critique...\n",
            "  - Refining Deepseek's critique...\n",
            "  - Refining Qwen's critique...\n",
            "\n",
            "SYNTHESIS: Using GPT-OSS to create final unified critique...\n",
            "\n",
            "============================================================\n",
            "DELPHI PROCESS COMPLETE\n",
            "============================================================\n",
            "\n",
            "\n",
            "FINAL CRITIQUE:\n",
            "------------------------------------------------------------\n",
            "**FINAL CRITIQUE – “The Intelligence Age” (September 23 2024)**  \n",
            "*Synthesised from the Delphi‑style expert panel (LLAMA, DEEPSEEK, QWEN) after two rounds of independent and collaborative review.*\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Executive Summary  \n",
            "\n",
            "The manuscript offers an inspiring, historically‑anchored narrative that positions deep‑learning breakthroughs as the catalyst for a forthcoming “Intelligence Age.”  Its prose is clear, its central thesis (“deep learning worked”) memorable, and it supplies relatable use‑case sketches (AI tutors, health coordinators, software‑creation assistants).  The piece also signals awareness of societal risks—job disruption, inequality, and geopolitical competition—thereby avoiding a wholly naïve techno‑utopia.\n",
            "\n",
            "However, the document functions more as a manifesto than a rigorously substantiated forecast.  Across all three experts the following recurrent deficiencies were identified:\n",
            "\n",
            "* **Evidence Gap** – claims about AI‑driven prosperity, climate remediation, and super‑intelligence are presented without empirical support, case‑study backing, or quantitative grounding.  \n",
            "* **Over‑optimistic Technological Determinism** – the narrative treats scaling of current deep‑learning models as a guaranteed, linear path to super‑intelligence, ignoring known limits, possible paradigm shifts, and the decisive role of social‑political choices.  \n",
            "* **Superficial Risk & Governance Treatment** – while risks are mentioned, the analysis lacks depth, concrete mitigation strategies, and a framework for accountability, transparency, and fairness.  \n",
            "* **Narrow Technical Focus** – the manuscript centres exclusively on large‑scale deep learning, omitting alternative AI research strands (symbolic, neuro‑symbolic, neuromorphic, reinforcement‑learning‑based) that could be pivotal for the next leap.  \n",
            "* **Missing Diversity of Perspectives & Contextual Nuance** – cultural, geopolitical, and interdisciplinary viewpoints are under‑represented, limiting the paper’s relevance to a global audience.  \n",
            "* **Unclear Timeline** – the “few thousand days” horizon for super‑intelligence is offered without citation or discussion of uncertainty.\n",
            "\n",
            "The iterative review process sharpened these observations, added nuance (e.g., energy‑efficiency concerns, policy‑level recommendations), and produced a more balanced set of actionable improvements.\n",
            "\n",
            "---\n",
            "\n",
            "## 2. Consensus Findings (Agreement Across All Experts)\n",
            "\n",
            "| Area | Consensus Point |\n",
            "|------|-----------------|\n",
            "| **Vision & Narrative** | The document’s visionary framing, historical continuity, and compelling storytelling are strong assets. |\n",
            "| **Clarity & Accessibility** | Language is non‑technical enough for a broad audience; the central thesis is memorable. |\n",
            "| **Societal Impact Emphasis** | The piece highlights potential benefits for health, education, and productivity. |\n",
            "| **Call to Action** | It rightly urges proactive investment and responsible stewardship. |\n",
            "| **Evidence Deficiency** | All experts agree that concrete data, case studies, or citations are largely absent. |\n",
            "| **Over‑optimism & Determinism** | The manuscript over‑states the inevitability of AI progress and under‑states uncertainty. |\n",
            "| **Risk Analysis** | Risks are mentioned but not explored in depth; mitigation strategies are missing. |\n",
            "| **Perspective Diversity** | The work reflects a single (largely Western/technical) viewpoint; broader stakeholder inclusion is needed. |\n",
            "| **Timeline Ambiguity** | The super‑intelligence horizon is vague and unsupported. |\n",
            "| **Alternative AI Paths** | Focusing solely on deep learning limits the paper’s technical completeness. |\n",
            "\n",
            "---\n",
            "\n",
            "## 3. Key Evolution from Round 1 to Round 2  \n",
            "\n",
            "| Dimension | Round 1 (Independent)\n",
            "\n",
            "✓ Results saved to delphi_critique.docx\n"
          ]
        }
      ]
    }
  ]
}
